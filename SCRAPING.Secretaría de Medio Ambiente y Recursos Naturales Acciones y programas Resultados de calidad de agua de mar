import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup

# Ruta de tu chromedriver
CHROMEDRIVER_PATH = '/Users...'  # Ajusta seg√∫n corresponda

# 1. Carga de datos
file_path = '/Users/......csv'# Ajusta seg√∫n tu ruta del .csv
df = pd.read_csv(file_path, dtype={
    'Latitud norte': str,
    'Longitud oeste': str,
    'NMP/100 ml': str,
    'Clasificaci√≥n': str
# Ajusta seg√∫n tus columnas de tu csv, en mi caso son estas
})

# 2. Asegurar columnas
for col in ['Latitud norte','Longitud oeste', 'NMP/100 ml', 'Clasificaci√≥n']:
    if col not in df.columns:
        df[col] = ''

# 3. Configuraci√≥n de Selenium (Headless Chrome)
options = Options()
options.add_argument("--headless")
options.add_argument("--disable-gpu")
options.add_argument("--no-sandbox")

service = Service(CHROMEDRIVER_PATH)

# 4. Funci√≥n de scraping
def scrap_semarnat_selenium(driver, url, sitio_playa):
    try:
        driver.get(url)
        # Espera hasta que la tabla est√© presente en el DOM
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table#example tbody tr"))
        )
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        tabla = soup.find('table', id='example')
        if not tabla:
            print(f"‚ùå No se encontr√≥ la tabla en {url}")
            return None

        for tr in tabla.select('tbody > tr'):
            tds = tr.find_all('td')
            sitio = tds[1].get_text(strip=True)
            if sitio.lower() == sitio_playa.lower():
                return {

                    'Latitud norte': tds[2].get_text(strip=True),
                    'Longitud oeste': tds[3].get_text(strip=True),
                    'NMP/100 ml': tds[5].get_text(strip=True),
                    'Clasificaci√≥n': tds[6].get_text(strip=True),
                }
        print(f"‚ùå Playa '{sitio_playa}' no encontrada en la tabla.")
    except Exception as e:
        print(f"‚ùå Error al scrapear {url}: {e}")
    return None

# 5. Abrir y procesar con contexto para asegurar cierre
with webdriver.Chrome(service=service, options=options) as driver:
    for idx, row in df.iterrows():
        url = row.get('Semarnat_URL', '')
        sitio = row.get('Sitio de muestreo', '')
        if not url:
            print(f"‚ö†Ô∏è Sin URL para '{sitio}', omitiendo...")
            continue
        print(f"üîé Procesando: {sitio}")
        datos = scrap_semarnat_selenium(driver, url, sitio)
        if datos:
            for campo, valor in datos.items():
                df.at[idx, campo] = valor

# 6. Guardar cambios
df.to_csv(file_path, index=False)
print("‚úÖ Base de datos actualizada con √©xito.")
